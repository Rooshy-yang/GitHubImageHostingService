# Dyna：一种基于模型的策略学习框架

我们知道无模型方法的优势在于，在能够提供足够多的环境交互数据的情况下算法能证明出policy能渐近收敛到最优解的。有模型的好处在于，能够模拟环境的状况来进行推断（利用模型作为环境来进行规划planning），进而减少环境交互过程不必要的方差波动，加速算法的运行效率。

Dyna的提出尝试将上述方法的优势结合起来：简述算法流程为：基于Q(s,a)与真实环境进行一次交互，更新学习模型（可选）；然后重复n次下述操作：从Buffer以前访问的状态中选取一个状态和行为带入到**模型**P中产生R和S'，然后更新Q(s,a)。然后返回到算法开始处。预测模型的训练通过过去的(r,s',s,a)作为训练集。

当然这里有一些优化的点：

1.Prioritized Dyna-Q 考虑随机选取某一个状态和动作会产生一些无效状态的探索，例如对于那些Q值变化不大的状态，我们倾向于“原路行进”，因此我们考虑利用优先队列的方法来选取状态和动作，启发函数可以设为P= $r + \gamma max(Q(s',a)) - Q (s,a)$。当超过一定阈值的状态行为对我们才考虑放入模型中。首先采用相应的行为策略产生R,S'，然后更新模型，计算启发函数以一定的阈值插入到队列中，从队列中弹出S,A输入到模型中产生R和S'，更新相应的Q函数，然后注意再循环更新一下会导致状态S的状态行为对的P值，插入队列中。返回到弹出队列步骤直到队列为空返回到算法开始。 具体见Sutton. RL_intro:Prioritized Sweeping Dyna-Q。

2.改进版Dyna-2框架。考虑Dyna 本身目的是根据实际经验来拟合一个模型，然后生成模拟数据，所以这种基于样本的规划的有效性取决于模型的准确性。我们可以考虑一种**基于样本的搜索**方法。因此该框架涉及两个模块：学习模块和搜索模块。

    搜索模块中，在模型中利用蒙特卡罗模拟在时间允许下尽可能对地产生基于同一个状态出发的轨迹（直观上看是一条以s状态为根的搜索树），然后计算这些轨迹的经验平均回报，作为(s,a)的值函数$Q_{mb}$，最后选择最大$Q$的那个动作作为真实动作与环境进行交互，注意一点，这里要特别注意决定真实动作的Q应该包含和环境进行交互的真实值函数$Q_{mf}$，也就是说Dyna-2实际上维护两个值函数（可以看成是瞬时记忆和长期记忆），利用这两个值函数来共同决定智能体的动作。

    学习模块就是采用后向$sarsa(\lambda))$算法与环境交互，每一步都调用搜索模块对$s$进行搜索，这里略过具体过程。

这里区别一下UCT ： MCTS 和UCB置信上限区间作为TreePolicy的算法

MCTS ： 从$s_1$出发选择一个动作产生下一个状态（叶子）$s_l$，然后使用行为策略（如$\epsilon-greedy$）产生一条轨迹并计算$Q(s_l,a)$和N，这个a是策略产生的，N是这个节点从$s_1$出发被访问过的次数。然后计算相应的分值如UCB，目的是为了平衡搜索和最大收益。然后更新这个轨迹上之前的节点的Q和N值
