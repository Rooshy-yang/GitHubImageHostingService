多智能体强化学习：

1.独立式学习：把其他的agent当作环境,可以利用单智能体的方法来解决。self-play

2.集中式学习：把所有的智能体联合起来成为一个联合策略。AlphaStar

3.集中训练分布执行：MADDPG，COMA， QMIX-like（比较火2021）

如果是零和博弈的话存在一个NashSolver，极大极小值的方法来做

Google 全球人工智能比赛--足球游戏---体育竞技这一方面会比较少一些。

多智能体存在问题：计算资源需求大，理论进展不足。



胡裕靖 网易伏羲实验室负责人。GameAI

背景

1.基于规则的一些方法，例如行为树（开发软件例如UE4），有限状态机。但是有一些不好的地方，就是没有泛化性，当行为树变得很大的时候不好维护。

2.利用神经网络，有些行为会比较诡异。

基于强化学习的解决方法：

模仿学习，设定学习目标和奖励函数，然后去训练。

self play 

ail教练系统



IMPALA  ->(PPO AC) 

向量分组，然后多头输入到神经网络里：多头的意思是先把特征分组放进多个不同网络里面，然后用来



shaping reward： 人为设置一些奖励，风格确实会变化，但是最终的reward会减少(RL football game given)，可以放心地给网络selfplay,他能够自己找到一些比较好的策略。

mask some unnecessary actions (exploration)

curriculum learning ： 在不同的位置进行训练

PVE，神经网络动态拓展，球员站位信息（GNN？）

不同层次的策略分别学习（射门和传球----> 跑位，配合）

surgery technique : 将特征进行动态拓展？

