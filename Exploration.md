---
title = 强化学习的探索问题
 zhuhu-url: 
 zhihu-title-image: []
 zhihu-tags:强化学习
---
# Exploration

探索是强化学习中很大的问题。

为什么要研究探索？

1. 一个角度是，探索是为了发现一个更高奖励的策略，这个策略需要暂时做一些复杂的没有短期收益的动作来保证未来收益最大化。例如在Atari游戏中，Q- learning在Montezuma's revenge 游戏中智能体会直接碰骷髅头送死而不是拿到钥匙去开门（因为这个游戏奖励的设置是每走一步都减少，碰到钥匙和成功开门加更高的奖励，但是碰到骷髅头会死掉重开，奖励为0）。
2. 另一个角度是，探索是为了尝试一些新的动作以发现一些新的高奖励策略。
3. 进一步地如果单单依靠增加噪声的方式进行探索(如$\epsilon-greedy$等)，虽然能保证充分探索，但是需要海量的数据支持，数据利用率也不高。所以需要提供即能从数学证明有后悔值bound，也能在实践上很好平衡好探索和利用的关系的算法。

从是否可以从理论上可解的角度看探索问题：

![image-20220604190208153](../../../../Library/Application Support/typora-user-images/image-20220604190208153.png)

[cs285]判断一个探索问题是否是理论可解的指标是能不能形式化成POMDP（部分可观测马尔可夫链）

探索问题可以抽象成Bandit来形式化描述：

`<img src="../../../../Library/Application Support/typora-user-images/image-20220604195106470.png" alt="image-20220604195106470" style="zoom: 50%;" />`

其中奖励是服从以theta作为参数的概率分布。

一个用来评估探索策略好坏的指标---regret，用来表示在T时间步下该探索策略与已知的最优策略的差距：

$$
Reg(T) = TE[r(a^*)] - \sum^T_{t=1}r(a_t)
$$

其中$a^*$是策略能考虑的最大收益动作，$a_t$表示策略实际做出的动作。

我们希望后悔值尽可能地小，但是后悔值可能会随着T的增加而增加，所以我们要尽可能地去避免在策略运行后期能够提供和最优策略差不多的价值。

举例一些方法：（感觉每一种都可以开一章来讲）

1. UCB1: $\pi = argmax Q +C \sqrt{\frac {2ln T}{N(s)}}$ ， $T$是时间戳，$N$是该状态的访问次数。接下来讨论置信区间上界的推导:

   首先回答为什么要设定置信上限区间，置信上限区间的目的是提供在一定的置信度下该分布下能够达到的上限，可以用来表示一个分布能够取到的可能的最大值。如果分布已知，那么是可以求出相应的置信区间的，例如关于贝叶斯UCB方法[Emilie,1988]就是假设回报函数服从高斯分布$R_a(r) = N(r; \mu,\delta^2)$，进而求解出行为选择的方法为：$a_t = argmax \mu_a + \frac{c \delta_a}{\sqrt{N(a)}}$。而对于未知分布的置信区间求法如下：令$X_1,...X_n$为独立随机变量，且$X_t \in [0,1]$，$\overline X_t$为这些随机变量的均值，$E[X]$为随机变量的期望。那么有：

   $$
   p(E[X] > \overline X_t + u) <= e^{-2tu^2}
   $$

   根据我们的bandits问题可以得到:

   $$
   p(Q(a) > \widehat Q_t(a) + U_t(a)) <= e^{-2N_t(a) U_t^2(a)}
   $$

   其中我们的$\widehat Q_t(a)$表示时间t步下实际的行为价值，$N_t(a)$表示t步下做过动作行为$a$的次数。右边的式子表示超过我们设置的置信区间上界的概率p，那么我们令p = 右式换算出我们需要的$U_t(a)$就等于:$\sqrt {\frac{-log p}{2N_t(a)}}$ 。我们让概率随着时间的增长而减少:$p = t^{-4}$。最终就得到了式子$\pi = argmax Q +C \sqrt{\frac {2ln T}{N(s)}}$ 。UCB1的后悔值是O(log(T))的。
   
   除了UCB1的探索奖励以外还有一些其他的奖励方法，例如：$\sqrt \frac 1 {N(s)}$ [strehl.MBIR-EB]; $\frac 1 {N(s)}$[Kolter. BEB] .
2. 乐观初始值估计，一开始为每一个臂位设置较高的价值Q，然后逐渐去更新相应的价值。
3. Posterior Sampling or Thompson sampling：以贝叶斯定理为中心为每一个臂位定义一个概率分布模型。Thompson sampling对每一个臂位采用beta分布，通过每一次的交互来调整a,b使得臂位满足一定的概率分布，进而采样出最有可能产生最高回报的动作。这样的算法被证明有一个后悔值上界[Agrawal,2012]。**待完续...**
4. pseudo count。观察UCB的探索项 $B(N(s)) = \sqrt{\frac {2ln T}{N(s)}}$  可以看成是一个基于计数的探索奖励方法，然而会存在一些问题，例如在高维空间或者连续的空间中，对状态s的计数就会非常稀疏以至于效果很差。但我们依然可以发现很多的状态(观察)其实是有一定的相似性的，我们可以利用一个概率模型$p_\theta(s)$来判断该当前时刻/帧为状态（观察）$s$的概率大小。[Bellemare. Unifying count-based Exploration and Intrinsic Motivation]给出了基于概率的计数$\N(s) = \frac{p(x)(1 - p'(x))} {p'(x) - p(x)}$,其中$p'(x)$表示观察到当前状态s后到概率。具体见另一篇文章。
5. 基于信息增益(information gain) ：

有很多种估计信息增益的表达式，例如在当前状态下估计错出某一个动作得到的状态可能得到的信息增益是多少来判断是否做出该动作。**待完续...**
